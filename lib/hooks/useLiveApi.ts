import { useCallback, useEffect, useMemo, useRef, useState } from "react";
import { GenAILiveClient } from "@/lib/voice/genai-live-client";
import { LiveClientOptions } from "@/types/voice";
import { AudioStreamer } from "@/lib/voice/audio-streamer";
import { audioContext } from "@/lib/voice/utils";
import VolMeterWorket from "@/lib/voice/worklets/vol-meter";
import { LiveConnectConfig, Modality, MediaResolution } from "@google/genai";

export type UseLiveAPIResults = {
  client: GenAILiveClient;
  setConfig: (config: LiveConnectConfig) => void;
  config: LiveConnectConfig;
  model: string;
  setModel: (model: string) => void;
  connected: boolean;
  connect: () => Promise<void>;
  disconnect: () => Promise<void>;
  volume: number;
};

export function useLiveAPI(options: LiveClientOptions): UseLiveAPIResults {
  const client = useMemo(() => new GenAILiveClient(options), [options]);
  const audioStreamerRef = useRef<AudioStreamer | null>(null);

  const [model, setModel] = useState<string>("models/gemini-2.5-flash-preview-native-audio-dialog");
  const [config, setConfig] = useState<LiveConnectConfig>({
    responseModalities: [Modality.AUDIO],
    mediaResolution: MediaResolution.MEDIA_RESOLUTION_MEDIUM,
    speechConfig: {
      languageCode: "ja-JP",
      voiceConfig: {
        prebuiltVoiceConfig: {
          voiceName: "Zephyr",
        },
      },
    },
    contextWindowCompression: {
      triggerTokens: "25600",
      slidingWindow: { targetTokens: "12800" },
    },
  });
  const [connected, setConnected] = useState(false);
  const [volume, setVolume] = useState(0);

  // ãƒ­ã‚°é–¢æ•°ã‚’å–å¾—ï¼ˆVoiceChatã®logé–¢æ•°ã‚’ä½¿ç”¨ï¼‰
  const logToVoiceChat = (msg: string, level: string = 'info') => {
    // ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«logé–¢æ•°ãŒã‚ã‚‹å ´åˆã¯ä½¿ç”¨
    if (typeof window !== 'undefined' && (window as any).voiceChatLog) {
      (window as any).voiceChatLog(msg, level);
    } else {
      console.log(`[useLiveAPI] ${msg}`);
    }
  };

  // addWorkletç”¨ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’useCallbackã§ãƒ¡ãƒ¢åŒ–
  const handleVolume = useCallback((ev: any) => {
    setVolume(ev.data.volume);
  }, []);

  useEffect(() => {
    const onOpen = () => {
      console.log('[useLiveAPI] onOpenç™ºç«');
      if (typeof window !== 'undefined' && (window as any).voiceChatLog) {
        (window as any).voiceChatLog('ğŸ¯ [DEBUG] useLiveAPI: onOpenã‚¤ãƒ™ãƒ³ãƒˆç™ºç« - connectedã‚’trueã«è¨­å®š', 'success');
      }
      setConnected(true);
    };

    const onClose = () => {
      console.log('[useLiveAPI] onCloseç™ºç«');
      if (typeof window !== 'undefined' && (window as any).voiceChatLog) {
        (window as any).voiceChatLog('ğŸ¯ [DEBUG] useLiveAPI: onCloseã‚¤ãƒ™ãƒ³ãƒˆç™ºç« - connectedã‚’falseã«è¨­å®š', 'warn');
      }
      setConnected(false);
    };

    const onError = (error: ErrorEvent) => {
      console.error('[useLiveAPI] onErrorç™ºç«', error);
      if (typeof window !== 'undefined' && (window as any).voiceChatLog) {
        (window as any).voiceChatLog(`ğŸ¯ [DEBUG] useLiveAPI: onErrorã‚¤ãƒ™ãƒ³ãƒˆç™ºç« - ${error.message}`, 'error');
      }
    };

    const stopAudioStreamer = () => audioStreamerRef.current?.stop();

    const onAudio = (data: ArrayBuffer) =>
      audioStreamerRef.current?.addPCM16(new Uint8Array(data));

    client
      .on("error", onError)
      .on("open", onOpen)
      .on("close", onClose)
      .on("interrupted", stopAudioStreamer)
      .on("audio", onAudio);

    return () => {
      client
        .off("error", onError)
        .off("open", onOpen)
        .off("close", onClose)
        .off("interrupted", stopAudioStreamer)
        .off("audio", onAudio)
        .disconnect();
    };
  }, [client]);

  const connect = useCallback(async () => {
    logToVoiceChat('ğŸ¯ [DEBUG] useLiveAPI.connect()é–‹å§‹', 'api');
    console.log('[useLiveAPI] connecté–‹å§‹');
    if (!config) {
      throw new Error("config has not been set");
    }
    try {
      if (!audioStreamerRef.current) {
        logToVoiceChat('[useLiveAPI] audioContextåˆæœŸåŒ–é–‹å§‹', 'api');
        console.log('[useLiveAPI] audioContextåˆæœŸåŒ–é–‹å§‹');
        const audioCtx = await audioContext({ id: "audio-out" });
        logToVoiceChat('[useLiveAPI] audioContextåˆæœŸåŒ–æˆåŠŸ', 'success');
        console.log('[useLiveAPI] audioContextåˆæœŸåŒ–æˆåŠŸ', audioCtx);
        audioStreamerRef.current = new AudioStreamer(audioCtx);
        try {
          console.log('[useLiveAPI] addWorkleté–‹å§‹');
          await audioStreamerRef.current.addWorklet<any>("vumeter-out", VolMeterWorket, handleVolume);
          console.log('[useLiveAPI] addWorkletæˆåŠŸ');
        } catch (e) {
          logToVoiceChat(`[useLiveAPI] addWorkletå¤±æ•—: ${e}`, 'error');
          console.error('[useLiveAPI] addWorkletå¤±æ•—', e);
        }
      }
      console.log("ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«:", model);
      console.log("âš™ï¸ æ¥ç¶šè¨­å®š:", {
        responseModalities: config.responseModalities,
        languageCode: config.speechConfig?.languageCode,
        voiceName: config.speechConfig?.voiceConfig?.prebuiltVoiceConfig?.voiceName
      });
      try {
        client.disconnect();
        logToVoiceChat('ğŸ¯ [DEBUG] client.connect()ç›´å‰', 'api');
        console.log('[useLiveAPI] client.connecté–‹å§‹');
        await client.connect(model, config);
        logToVoiceChat('ğŸ¯ [DEBUG] client.connect()æˆåŠŸ', 'success');
        console.log('[useLiveAPI] client.connectæˆåŠŸ');
      } catch (e) {
        logToVoiceChat(`ğŸ¯ [DEBUG] client.connect()ã‚¨ãƒ©ãƒ¼: ${e}`, 'error');
        console.error('[useLiveAPI] client.connectå¤±æ•—', e);
        throw e;
      }
    } catch (e) {
      logToVoiceChat(`ğŸ¯ [DEBUG] useLiveAPI.connect()å…¨ä½“ã‚¨ãƒ©ãƒ¼: ${e}`, 'error');
      console.error('[useLiveAPI] connectå…¨ä½“ã§å¤±æ•—', e);
      throw e;
    }
  }, [client, config, model, handleVolume, logToVoiceChat]);

  const disconnect = useCallback(async () => {
    client.disconnect();
    setConnected(false);
  }, [setConnected, client]);

  return {
    client,
    config,
    setConfig,
    model,
    setModel,
    connected,
    connect,
    disconnect,
    volume,
  };
} 